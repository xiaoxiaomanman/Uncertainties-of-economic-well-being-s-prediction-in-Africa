{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisites\" data-toc-modified-id=\"Pre-requisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisites</a></span></li><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Instructions</a></span></li><li><span><a href=\"#Imports-and-Constants\" data-toc-modified-id=\"Imports-and-Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports and Constants</a></span></li><li><span><a href=\"#Constants\" data-toc-modified-id=\"Constants-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Constants</a></span></li><li><span><a href=\"#Export-Images\" data-toc-modified-id=\"Export-Images-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Export Images</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Register a Google account at [https://code.earthengine.google.com](https://code.earthengine.google.com). This process may take a couple of days. Without registration, the `ee.Initialize()` command below will throw an error message.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook exports Landsat satellite image composites from Google Earth Engine. The images are saved in gzipped TFRecord format (`*.tfrecord.gz`). The exported images take up a significant amount of storage space. Before exporting, make sure you have enough storage space.\n",
    "\n",
    "In this project, we download satellite images corresponding to three different datasets:\n",
    "\n",
    "- **DHS**: 19,669 clusters from DHS surveys, for which we predict cross-sectional (*i.e.*, static in time) cluster-level asset wealth\n",
    "- **DHSNL**: 260,415 locations sampled near DHS survey locations, for which we train transfer learning models to predict nightlights values\n",
    "- **LSMS**: 2,913 clusters from LSMS surveys, for which we predict changes in cluster-level asset wealth over time\n",
    "\n",
    "|       | Storage  | Expected Export Time\n",
    "|-------|----------|---------------------\n",
    "| DHS   | ~16.0 GB | ~24h\n",
    "| LSMS  |  ~2.5 GB | ~10h\n",
    "| DHSNL |  ~240 GB | ~72h\n",
    "\n",
    "By default, this notebook exports images to Google Drive. If you instead prefer to export images to Google Cloud Storage (GCS), change the `EXPORT` constant below to `'gcs'` and set `BUCKET` to the desired GCS bucket name. The images are exported to the following locations:\n",
    "\n",
    "|       | Google Drive (default) | GCS\n",
    "|-------|:-----------------------|:---\n",
    "| DHS   | `dhs_tfrecords_raw/`   | `{BUCKET}/dhs_tfrecords_raw/`\n",
    "| DHSNL | `dhsnl_tfrecords_raw/` | `{BUCKET}/dhsnl_tfrecords_raw/`\n",
    "| LSMS  | `lsms_tfrecords_raw/`  | `{BUCKET}/lsms_tfrecords_raw/`\n",
    "\n",
    "Once the images have finished exporting, download the exported TFRecord files to the following folders:\n",
    "\n",
    "- DHS: `data/dhs_tfrecords_raw/`\n",
    "- DHSNL: `data/dhsnl_tfrecords_raw/`\n",
    "- LSMS: `data/lsms_tfrecords_raw/`\n",
    "\n",
    "After downloading the TFRecord files, the `data/` directory should look as follows, where `XX` depends on the `CHUNK_SIZE` parameter used:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords_raw/\n",
    "        angola_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015_XX.tfrecord.gz\n",
    "    dhsnl_tfrecords_raw/\n",
    "        angola_2010_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2016_XX.tfrecord.gz\n",
    "    lsms_tfrecords_raw/\n",
    "        ethiopia_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        uganda_2013_XX.tfrecord.gz\n",
    "```\n",
    "\n",
    "After finishing this notebook, move on to [1_process_tfrecords.ipynb](./1_process_tfrecords.ipynb) for next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change directory to repo root, and verify\n",
    "#%cd '../'\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm.auto'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-90617be9ec1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mee_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\africa_poverty_clean-main\\preprocessing\\ee_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm.auto'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import Any, Optional\n",
    "\n",
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessing import ee_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the Earth Engine API, you must perform a one-time authentication that authorizes access to Earth Engine on behalf of your Google account you registered at [https://code.earthengine.google.com](https://code.earthengine.google.com). The authentication process saves a credentials file to `$HOME/.config/earthengine/credentials` for future use.\n",
    "\n",
    "The following command `ee.Authenticate()` runs the authentication process. Once you successfully authenticate, you may comment out this command because you should not need to authenticate again in the future, unless you delete the credentials file. If you do not authenticate, the subsequent `ee.Initialize()` command below will fail.\n",
    "\n",
    "For more information, see [https://developers.google.com/earth-engine/python_install-conda.html](https://developers.google.com/earth-engine/python_install-conda.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()  # initialize the Earth Engine API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADAPT THESE PARAMETERS ==========\n",
    "\n",
    "# To export to Google Drive, uncomment the next 2 lines\n",
    "EXPORT = 'drive'\n",
    "BUCKET = None\n",
    "\n",
    "# To export to Google Cloud Storage (GCS), uncomment the next 2 lines\n",
    "# and set the bucket to the desired bucket name\n",
    "# EXPORT = 'gcs'\n",
    "# BUCKET = 'mybucket'\n",
    "\n",
    "# export location parameters\n",
    "DHS_EXPORT_FOLDER = 'dhs_tfrecords_raw'\n",
    "DHSNL_EXPORT_FOLDER = 'dhsnl_tfrecords_raw'\n",
    "LSMS_EXPORT_FOLDER = 'lsms_tfrecords_raw'\n",
    "\n",
    "# Set CHUNK_SIZE to None to export a single TFRecord file per (country, year). However,\n",
    "# this may fail if it exceeds Google Earth Engine memory limits. Decrease CHUNK_SIZE\n",
    "# to a small number (<= 50) until Google Earth Engine stops reporting memory errors\n",
    "CHUNK_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DO NOT MODIFY THESE ==========\n",
    "\n",
    "# input data paths\n",
    "DHS_CSV_PATH = 'data/dhs_clusters.csv'\n",
    "DHSNL_CSV_PATH = 'data/dhsnl_locs.csv'\n",
    "LSMS_CSV_PATH = 'data/lsms_clusters.csv'\n",
    "\n",
    "# band names\n",
    "MS_BANDS = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "# image parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "SCALE = 30                # export resolution: 30m/px\n",
    "EXPORT_TILE_RADIUS = 127  # image dimension = (2*EXPORT_TILE_RADIUS) + 1 = 255px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(df: pd.DataFrame,\n",
    "                  country: str,\n",
    "                  year: int,\n",
    "                  export_folder: str,\n",
    "                  chunk_size: Optional[int] = None\n",
    "                  ) -> dict[tuple[str, str, int, int], ee.batch.Task]:\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
    "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
    "\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
    "    '''\n",
    "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(subset_df)\n",
    "    num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
    "    tasks = {}\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
    "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
    "        start_date, end_date = ee_utils.surveyyear_to_range(year)\n",
    "\n",
    "        # create 3-year Landsat composite image\n",
    "        roi = fc.geometry() # 剥离属性信息 只有空间信息\n",
    "        imgcol = ee_utils.LandsatSR(roi, start_date=start_date, end_date=end_date).merged\n",
    "        imgcol = imgcol.map(ee_utils.mask_qaclear).select(MS_BANDS)\n",
    "        img = imgcol.median()\n",
    "\n",
    "        # add nightlights, latitude, and longitude bands\n",
    "        img = ee_utils.add_latlon(img)\n",
    "        img = img.addBands(ee_utils.composite_nl(year))\n",
    "\n",
    "        fname = f'{country}_{year}_{i:02d}'\n",
    "        tasks[(export_folder, country, year, i)] = ee_utils.get_array_patches(\n",
    "            img=img, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "            points=fc, export=EXPORT,\n",
    "            prefix=export_folder, fname=fname,\n",
    "            bucket=BUCKET)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks: dict[tuple[str, str, int, int], ee.batch.Task] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False)\n",
    "dhs_surveys = list(dhs_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in dhs_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=dhs_df, country=country, year=year,\n",
    "        export_folder=DHS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhsnl_df = pd.read_csv(DHSNL_CSV_PATH, float_precision='high', index_col=False)\n",
    "dhsnl_surveys = list(dhsnl_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in dhsnl_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=dhsnl_df, country=country, year=year,\n",
    "        export_folder=DHSNL_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsms_df = pd.read_csv(LSMS_CSV_PATH, float_precision='high', index_col=False)\n",
    "lsms_surveys = list(lsms_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in lsms_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=lsms_df, country=country, year=year,\n",
    "        export_folder=LSMS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on the status of each export task at [https://code.earthengine.google.com/](https://code.earthengine.google.com/), or run the following cell which checks every minute. Once all tasks have completed, download the DHS TFRecord files to `data/dhs_tfrecords_raw/`, DHSNL TFRecord files to `data/dhsnl_tfrecords_raw/`, and LSMS TFRecord files to `data/lsms_tfrecords_raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_utils.wait_on_tasks(tasks, poll_interval=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r3py3",
   "language": "python",
   "name": "r3py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
